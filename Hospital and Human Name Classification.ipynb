{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9041e98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First we need to import libraries like-\n",
    "# 1--> Pandas for mostly used for data analysis tasks in Python .Pandas library works well for numeric, alphabets, and heterogeneous types of data simultaneously.\n",
    "# 2--> NumPy for mostly used for working with Numerical values as it makes it easy to apply mathematical functions.\n",
    "# 3--> tensorflow for Being an Open-Source library for deep learning and machine learning, TensorFlow plays a role in text-based applications, image recognition, voice search, and many more.\n",
    "# 4--> Keras sequential model is suitable for analysis and comparison of simple neural network-oriented models which comprises layers and their associated data using top to bottom flow. It makes use of a single set of input as to value and a single set of output as per flow.\n",
    "# Here we use LSTM because --\n",
    "   #LSTM (Long Short-Term Memory) network is a type of RNN (Recurrent Neural Network) that is widely used for learning sequential data prediction problems. As every other neural network LSTM also has some layers which help it to learn and recognize the pattern for better performance.\n",
    "# Dense layer is used for implementing a dense layer that involves the neurons receiving the input from all the previous neurons that help implement the neural networks.\n",
    "# Embeddings make it easier to do machine learning on large inputs like sparse vectors representing words. Ideally, an embedding captures some of the semantics of the input by placing semantically similar inputs close together in the embedding space. An embedding can be learned and reused across models.\n",
    "# Adam uses estimations of first and second moments of gradient to adapt the learning rate for each weight of the neural network.\n",
    "\n",
    "\n",
  
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer # Tokenizer is used for Transforms each text in texts to a sequence of integers.\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences # pad_sequences are used to pad the sequences with the same length\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load hospital names dataset\n",
    "df = pd.read_csv('Hospital.csv')\n",
    "\n",
    "# Tokenize hospital names\n",
    "tokenizer = Tokenizer(lower=True)\n",
    "tokenizer.fit_on_texts(df.hosp_name)\n",
    "tokenized_hospitals = tokenizer.texts_to_sequences(df.hosp_name)\n",
    "df.head()\n",
    "\n",
    "# Set max sequence length\n",
    "max_length = max([len(x) for x in tokenized_hospitals])\n",
    "\n",
    "max_length\n",
    "\n",
    "# Pad tokenized hospital names\n",
    "padded_hospitals = pad_sequences(tokenized_hospitals, maxlen=max_length, padding='post')\n",
    "\n",
    "# Define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index)+1, 64, input_length=max_length))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(len(tokenizer.word_index)+1, activation='softmax'))\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",

    "# Generate new hospital names\n",
    "start_word = \"Hospital\"\n",
    "start_word_tokenized = tokenizer.texts_to_sequences([start_word])[0]\n",
    "start_word_padded = pad_sequences([start_word_tokenized], maxlen=max_length)\n",
    "generated_word = model.predict(start_word_padded, verbose=0)[0]\n",
    "generated_word_index = np.argmax(generated_word)\n",
    "generated_word_tokenized = tokenizer.index_word[generated_word_index]\n",
    "\n",
    "# Print generated hospital names\n",
    "generated_hospitals = []\n",
    "for i in range(10):\n",
    "    start_word_tokenized = tokenizer.texts_to_sequences([generated_word_tokenized])[0]\n",
    "    start_word_padded = pad_sequences([start_word_tokenized], maxlen=max_length)\n",
    "    generated_word = model.predict(start_word_padded, verbose=0)[0]\n",
    "    generated_word_index = np.argmax(generated_word)\n",
    "    generated_word_tokenized = tokenizer.index_word[generated_word_index]\n",
    "    generated_hospitals.append(generated_word_tokenized)\n",
    "\n",
    "print(generated_hospitals)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv('Hospital.csv')\n",
    "data.head()\n",
    "\n",
    "\n",
    "# Tokenize the words for each hospital name\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(data['hosp_name'])\n",
    "\n",
    "# Generate sequences for each hospital name\n",
    "sequences = tokenizer.texts_to_sequences(data['hosp_name'])\n",
    "\n",
    "\n",
    "# Get the vocabulary size\n",
    "vocabulary_size = len(tokenizer.word_index)+1\n",
    "\n",
    "# Generate X and y\n",
    "X = pad_sequences(sequences, maxlen=50)\n",
    "y = np.array([1] * X.shape[0])\n",
    "\n",
    "\n",
    "# Create model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocabulary_size, output_dim=50, input_length=50))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X, y, batch_size=128, epochs=20, verbose=1)\n",
    "\n",
    "# Generate new hospital names\n",
    "seed_text = \"Hosp\"\n",
    "for _ in range(20):\n",
    "    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "    token_list = pad_sequences([token_list], maxlen=50, padding='pre')\n",
    "    predicted = (model.predict(token_list) > 0.5).astype(\"int32\")\n",
    "    output_word = \"\"\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == predicted:\n",
    "            output_word = word\n",
    "            break\n",
    "    seed_text += output_word\n",
    "\n",
    "print(seed_text)\n",
    "\n",
    "\n",

    "# making Data frame\n",
    "data_human = pd.read_csv('human_names.csv')\n",
    "data_hospital = pd.read_csv('Hospital.csv')\n",
    "\n",
    "# Classifying Human and Hospital Names Using Character-Level Deep Learning\n",
    "\n",
    "# Read the dataset\n",
    "data_human = pd.read_csv('human_names.csv')\n",
    "data_hospital = pd.read_csv('Hospital.csv')\n",
    "\n",
    "# Tokenize the words for each name\n",
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(data_human.Name)\n",
    "tokenizer.fit_on_texts(data_hospital.hosp_name)\n",
    "\n",
    "# Generate sequences for each name\n",
    "human_sequences = tokenizer.texts_to_sequences(data_human['Name'])\n",
    "hospital_sequences = tokenizer.texts_to_sequences(data_hospital['hosp_name'])\n",
    "\n",
    "\n",
    "# Test the model\n",
    "seed_text = \"jhon hospital\"\n",
    "token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "token_list = pad_sequences([token_list], maxlen=50, padding='pre')\n",
    "predicted=(model.predict(token_list) > 0.5).astype(\"int32\")\n",
    "if predicted == 0:\n",
    "    print(\"This is a human name\")\n",
    "else:\n",
    "    print(\"This is a hospital name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62d6138",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34acc164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e234cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ee06c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75409bef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f33ca0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
